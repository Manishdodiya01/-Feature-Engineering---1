{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bede7a48-3418-450a-9081-e8a443ed9863",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q1. What is the Filter method in feature selection, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f22dbf0-eacc-4c81-aa47-bd565150f4a2",
   "metadata": {},
   "source": [
    "The filter method is a feature selection technique used in machine learning and data analysis to select the most relevant features (variables or attributes) for a given problem. It operates independently of any specific machine learning algorithm and focuses on ranking features based on certain statistical measures or scores. The primary idea behind the filter method is to evaluate the intrinsic characteristics of individual features without considering the interaction between features.\n",
    "\n",
    "Here's how the filter method works:\n",
    "\n",
    "1. **Feature Scoring**: Each feature is scored or ranked using a specific metric that captures its relevance to the target variable or the overall dataset. The choice of scoring metric depends on the nature of the data (categorical or numerical) and the problem at hand. Common scoring metrics include:\n",
    "   - **Correlation**: Measures the linear relationship between a feature and the target variable.\n",
    "   - **Mutual Information**: Quantifies the amount of information one variable provides about another variable.\n",
    "   - **Chi-squared**: Used for categorical features to measure the independence between variables.\n",
    "   - **ANOVA (Analysis of Variance)**: Measures the variance between group means.\n",
    "   - **Information Gain**: Measures the reduction in uncertainty about the target variable after knowing the feature's value.\n",
    "\n",
    "2. **Ranking Features**: After calculating the scores for each feature, they are ranked in descending order. Features with higher scores are considered more relevant or informative.\n",
    "\n",
    "3. **Feature Selection**: A threshold or a fixed number of top-ranked features are selected based on the ranking. Features below the threshold may be discarded. This threshold can be determined through experimentation or domain knowledge.\n",
    "\n",
    "4. **Machine Learning**: The selected subset of features is then used as input to a machine learning algorithm for model training and evaluation. The reduced feature set aims to improve model performance by eliminating irrelevant or redundant features that may introduce noise or overfitting.\n",
    "\n",
    "Advantages of the filter method include its simplicity, efficiency, and ability to handle high-dimensional datasets. However, it does have some limitations, such as ignoring feature interactions and not considering the impact of feature selection on the performance of the specific machine learning algorithm being used.\n",
    "\n",
    "It's worth noting that while the filter method is a popular and widely used approach, there are other feature selection techniques like wrapper methods and embedded methods that take into account the performance of the chosen machine learning algorithm during feature selection. These methods can provide more accurate feature selection by considering the interaction between features and the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd4463a-6b76-4e20-8734-8e1fa5c3fbe9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q2. How does the Wrapper method differ from the Filter method in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a96bae2-068b-4be0-a8a6-857e11622bef",
   "metadata": {},
   "source": [
    "The wrapper method and the filter method are two distinct approaches to feature selection in machine learning. They differ in their underlying principles, workflows, and how they incorporate the machine learning algorithm into the feature selection process.\n",
    "\n",
    "**Wrapper Method**:\n",
    "\n",
    "1. **Evaluation Based on Model Performance**: In the wrapper method, the feature selection process is tightly integrated with the training and evaluation of a specific machine learning model. It treats feature selection as a part of the model selection process.\n",
    "\n",
    "2. **Subset Search**: The wrapper method explores different subsets of features and evaluates each subset's performance using cross-validation or a similar technique. It repeatedly trains and evaluates the model with different feature subsets to find the one that yields the best performance.\n",
    "\n",
    "3. **Interaction with the Learning Algorithm**: The wrapper method takes into account the specific machine learning algorithm being used. It considers how the choice of features affects the model's performance and aims to find the subset of features that maximizes the model's performance.\n",
    "\n",
    "4. **Computationally Intensive**: Since the wrapper method involves training and evaluating the model multiple times with different feature subsets, it can be computationally expensive and time-consuming, especially for large datasets.\n",
    "\n",
    "**Filter Method**:\n",
    "\n",
    "1. **Independent of Learning Algorithm**: The filter method is independent of the specific machine learning algorithm being used. It focuses on ranking features based on certain statistical measures or scores without considering the performance of a specific model.\n",
    "\n",
    "2. **Preprocessing Step**: The filter method is typically applied as a preprocessing step before model training. It aims to identify the most relevant features based on their intrinsic characteristics.\n",
    "\n",
    "3. **Efficiency**: The filter method is generally computationally efficient since it doesn't involve training and evaluating the model multiple times. It only calculates feature scores based on specific metrics.\n",
    "\n",
    "4. **Limited Interaction**: The filter method doesn't consider the interaction between features and the learning algorithm as closely as the wrapper method. It might not capture the nuances of how specific features contribute to the model's performance.\n",
    "\n",
    "In summary, the wrapper method and the filter method represent two different philosophies in feature selection. The wrapper method is more involved and tailored to a specific machine learning algorithm, as it evaluates feature subsets based on model performance. In contrast, the filter method is a more independent and efficient approach that ranks features based on their inherent properties without direct consideration of the learning algorithm's performance. Each method has its advantages and limitations, and the choice between them depends on the specific problem, dataset, and computational resources available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd495a9a-4ea0-498b-8f83-3284ded15c4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q3. What are some common techniques used in Embedded feature selection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b016da-f0e6-442d-9ee2-fb996d8a132d",
   "metadata": {},
   "source": [
    "Embedded feature selection methods are techniques that incorporate feature selection directly into the process of training a machine learning model. These methods aim to find the optimal subset of features by considering their impact on the model's performance during the learning process. Embedded methods often result in more efficient and effective feature selection compared to filter methods, as they exploit the interplay between features and the learning algorithm. Some common techniques used in embedded feature selection include:\n",
    "\n",
    "1. **LASSO (Least Absolute Shrinkage and Selection Operator)**: LASSO is a regularization technique commonly used in linear regression. It adds a penalty term to the loss function that encourages smaller coefficients for less relevant features, effectively leading to feature selection. As the model is trained, LASSO can drive the coefficients of irrelevant features to zero.\n",
    "\n",
    "2. **Ridge Regression**: Similar to LASSO, ridge regression is a regularization technique used in linear regression. It adds a penalty term to the loss function that limits the magnitude of coefficients. While it doesn't force coefficients to exactly zero like LASSO, it can shrink less relevant coefficients, effectively reducing their impact.\n",
    "\n",
    "3. **Elastic Net**: Elastic Net is a combination of LASSO and ridge regression. It combines both L1 (LASSO) and L2 (ridge) regularization terms to achieve a balance between feature selection and coefficient shrinkage.\n",
    "\n",
    "4. **Decision Trees and Ensembles**: Decision trees and ensemble methods like Random Forest and Gradient Boosting can perform feature selection implicitly. Decision trees split nodes based on feature importance, and ensemble methods aggregate the importance scores of individual trees. Features with low importance may be pruned from the model.\n",
    "\n",
    "5. **Regularized Models (e.g., Logistic Regression with L1 Penalty)**: Regularized versions of machine learning models, such as logistic regression with an L1 penalty, can be used for embedded feature selection. These penalties encourage the model to favor a sparse set of features, effectively performing feature selection.\n",
    "\n",
    "6. **Recursive Feature Elimination (RFE)**: While RFE can also be considered a wrapper method, it's often used as an embedded technique. It starts with all features, trains a model, and then recursively eliminates the least important features based on their coefficients or importance scores. This process continues until a desired number of features is reached.\n",
    "\n",
    "7. **Embedded Feature Selection Libraries**: Some machine learning libraries provide built-in methods for embedded feature selection. For example, scikit-learn in Python offers various regularized models and techniques that can be used for feature selection during model training.\n",
    "\n",
    "Embedded feature selection methods are advantageous because they explicitly consider the interaction between features and the learning algorithm, leading to a potentially more accurate and efficient feature subset. However, they can be more computationally intensive compared to filter methods, especially for large datasets or complex models. The choice of technique depends on the problem, dataset, and the specific machine learning algorithm being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2851f0-fa7b-401d-98da-e1de2657afe6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q4. What are some drawbacks of using the Filter method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51548a2b-d919-4e87-8ee8-66c6ea2faf00",
   "metadata": {},
   "source": [
    "While the filter method for feature selection has its advantages, it also comes with several drawbacks and limitations that researchers and practitioners should be aware of:\n",
    "\n",
    "1. **Independence from Learning Algorithm**: The filter method ranks features based on statistical measures without considering the impact of feature selection on the performance of a specific machine learning algorithm. This can lead to suboptimal feature subsets that might not be the best fit for the chosen algorithm.\n",
    "\n",
    "2. **No Feature Interaction Consideration**: The filter method evaluates features in isolation and doesn't account for potential interactions between features. It may fail to capture complex relationships that contribute to the predictive power of a model.\n",
    "\n",
    "3. **Limited to Intrinsic Characteristics**: The filter method relies solely on intrinsic properties of features, such as correlation or mutual information with the target variable. It might not capture domain-specific knowledge or context that could guide feature selection.\n",
    "\n",
    "4. **Redundant or Irrelevant Features**: The filter method might retain redundant or irrelevant features that have high scores based on the chosen metric but don't contribute significantly to the model's performance. This can introduce noise and potentially degrade model accuracy.\n",
    "\n",
    "5. **Threshold Sensitivity**: Setting an appropriate threshold for feature selection can be challenging. Choosing an overly strict threshold may lead to the exclusion of potentially useful features, while being too lenient might retain irrelevant features.\n",
    "\n",
    "6. **Doesn't Optimize Model Performance**: The filter method's main focus is on feature ranking and selection based on predefined metrics. It doesn't directly optimize the overall model performance, which could be particularly important for complex datasets and algorithms.\n",
    "\n",
    "7. **Sensitive to Feature Scaling**: Some filter methods, like correlation-based approaches, are sensitive to the scale of features. If features have different scales, the calculated correlations might be biased, leading to inaccurate feature rankings.\n",
    "\n",
    "8. **Assumes Independence of Features**: Many filter methods assume that features are independent of each other. In reality, features can often be correlated or exhibit multicollinearity, which can lead to misleading results.\n",
    "\n",
    "9. **Inability to Handle Nonlinear Relationships**: Filter methods are generally better suited for linear relationships between features and the target variable. They might struggle to capture complex nonlinear patterns.\n",
    "\n",
    "10. **Domain Knowledge and Context**: The filter method might not take into account domain-specific insights or expert knowledge about the problem, which could guide more effective feature selection.\n",
    "\n",
    "In summary, while the filter method offers a straightforward and efficient way to perform feature selection, it has limitations that can affect its ability to identify the most relevant features for a given problem. To mitigate these drawbacks, it's important to carefully choose appropriate scoring metrics, combine filter methods with domain knowledge, and consider more advanced techniques like wrapper or embedded methods that take the learning algorithm's performance into account."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91497a3c-13fd-4df4-aeb5-73b6024ebe0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9476ea-8f09-4ffd-8521-4d2e2b01e24d",
   "metadata": {},
   "source": [
    "The choice between using the Filter method or the Wrapper method for feature selection depends on various factors, including the characteristics of your data, the goals of your analysis, available computational resources, and the specific machine learning algorithm you intend to use. There are situations where the Filter method might be more suitable:\n",
    "\n",
    "1. **High-Dimensional Data**: When dealing with high-dimensional datasets where the number of features is much larger than the number of samples, the computational cost of wrapper methods can be prohibitive. Filter methods are computationally efficient and can handle such datasets more effectively.\n",
    "\n",
    "2. **Exploratory Data Analysis**: If your primary goal is to gain insights into the data, identify potentially important features, or perform a preliminary analysis, filter methods can provide a quick overview of feature relevance without requiring extensive model training.\n",
    "\n",
    "3. **Preprocessing Step**: The filter method can serve as a preprocessing step before applying more advanced feature selection or model training techniques. It helps narrow down the feature space and reduce noise before using a wrapper or embedded approach.\n",
    "\n",
    "4. **Simple Models**: When you're working with simple machine learning models or linear algorithms, and you don't expect complex feature interactions, the filter method's feature ranking based on statistical measures might be sufficient.\n",
    "\n",
    "5. **Domain Independence**: If you're not deeply familiar with the domain or don't have access to domain experts who can guide feature selection, the filter method can offer a data-driven starting point.\n",
    "\n",
    "6. **Feature Ranking**: If your main interest is in identifying the most relevant features based on their individual characteristics, rather than focusing on optimizing a specific machine learning model's performance, the filter method can be useful.\n",
    "\n",
    "7. **Large Datasets**: For large datasets, the filter method's computational efficiency can make it a practical choice, especially if running wrapper methods for extensive cross-validation or grid search is resource-intensive.\n",
    "\n",
    "8. **Resource Constraints**: If you have limited computational resources or time, the filter method can provide a reasonable compromise between efficiency and feature selection quality.\n",
    "\n",
    "9. **Baseline Model**: The filter method can help establish a baseline model quickly, allowing you to evaluate the feasibility of a machine learning approach before investing more time in complex feature selection techniques.\n",
    "\n",
    "Remember that the choice between filter and wrapper methods is not always a binary decision. Hybrid approaches can also be effective, where you start with filter methods to reduce the feature space and then use wrapper or embedded methods to fine-tune feature selection and model performance. It's essential to consider the specific characteristics of your data and the goals of your analysis when deciding which method to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2e65d0-3ca4-4fab-8b81-7d3d8b561424",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn. You are unsure of which features to include in the model because the dataset contains several different ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b9332-16db-47f9-8e8b-9dc85c6f3203",
   "metadata": {},
   "source": [
    "The Filter Method is a technique used for feature selection in machine learning, which involves selecting the most relevant attributes based on their intrinsic characteristics, rather than by training a model. Here's how you could use the Filter Method to choose pertinent attributes for your predictive model for customer churn in a telecom company:\n",
    "\n",
    "1. **Understand the Dataset:**\n",
    "   Start by thoroughly understanding the dataset and its features. This includes the meaning and significance of each attribute. Understand the data types (numeric, categorical, etc.) and potential relationships between different attributes.\n",
    "\n",
    "2. **Define a Metric:**\n",
    "   Choose an appropriate metric to evaluate the importance or relevance of each feature. For a predictive churn model, you might use metrics like correlation, mutual information, or chi-square depending on the type of features you have (continuous, categorical).\n",
    "\n",
    "3. **Calculate Relevance Scores:**\n",
    "   Calculate the chosen relevance scores for each feature in the dataset. The relevance scores will help you understand the individual contribution of each feature towards predicting customer churn.\n",
    "\n",
    "4. **Threshold Selection:**\n",
    "   Determine a threshold above which the features will be considered relevant. You can choose this threshold based on your domain knowledge, experimentation, or by observing the distribution of relevance scores.\n",
    "\n",
    "5. **Feature Selection:**\n",
    "   Select the features whose relevance scores exceed the threshold. These are the features that are deemed most pertinent for your predictive churn model.\n",
    "\n",
    "6. **Deal with Multicollinearity:**\n",
    "   Check for multicollinearity among the selected features. If two or more features are highly correlated, you might consider removing one of them to avoid redundancy and improve model interpretability.\n",
    "\n",
    "7. **Model Building:**\n",
    "   Once you've selected the pertinent features using the Filter Method, you can proceed to build your predictive model using these features. You can use various machine learning algorithms like logistic regression, random forests, or gradient boosting to create your churn prediction model.\n",
    "\n",
    "8. **Model Evaluation:**\n",
    "   Evaluate the model's performance using appropriate evaluation metrics like accuracy, precision, recall, F1-score, etc. You should perform cross-validation to ensure the model's generalization ability.\n",
    "\n",
    "9. **Iterate and Refine:**\n",
    "   If the model's performance is not satisfactory, you can consider iterating the process by adjusting the threshold for feature selection or by exploring different relevance metrics. You can also experiment with different combinations of features to find the best subset.\n",
    "\n",
    "10. **Interpretability and Business Insights:**\n",
    "    One of the advantages of the Filter Method is that it can lead to a more interpretable model by using only the most relevant features. This can also provide valuable insights into the factors that contribute to customer churn in the telecom company.\n",
    "\n",
    "Remember that the Filter Method is a preliminary step in feature selection. It's important to follow up with additional steps like wrapper methods (e.g., forward selection, backward elimination) and embedded methods (e.g., Lasso regression, tree-based feature importance) to further refine your feature set and improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7154e954-c2d0-4cde-a302-9c86dc324d55",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with many features, including player statistics and team rankings. Explain how you would use the Embedded method to select the most relevant features for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eac756-17b8-4a1a-bb82-066030070d53",
   "metadata": {},
   "source": [
    "Embedded methods are a type of feature selection technique that combines the feature selection process with the model training process. They work by fitting a machine learning algorithm to the data and selecting the most relevant features based on their importance within the context of the model. In your case, using an embedded method to select relevant features for predicting soccer match outcomes involves the following steps:\n",
    "\n",
    "1. **Choose a Model with Built-in Feature Importance:**\n",
    "   Select a machine learning algorithm that inherently provides a measure of feature importance during its training process. Common choices include tree-based algorithms like Random Forests, Gradient Boosting, and decision trees. These algorithms calculate feature importance scores based on how much they contribute to the model's predictive performance.\n",
    "\n",
    "2. **Preprocessing:**\n",
    "   Prepare your dataset by cleaning the data, handling missing values, and encoding categorical variables appropriately. This step is essential to ensure the quality of the training process.\n",
    "\n",
    "3. **Feature-Target Split:**\n",
    "   Divide your dataset into features (player statistics, team rankings) and the target variable (soccer match outcomes, such as win, loss, or draw).\n",
    "\n",
    "4. **Model Training:**\n",
    "   Train the chosen machine learning algorithm using your dataset. During the training process, the algorithm assigns weights to different features based on their relevance to predicting the target variable.\n",
    "\n",
    "5. **Retrieve Feature Importance:**\n",
    "   After the model is trained, you can extract feature importance scores from the model. These scores indicate how much each feature contributes to the model's predictive performance.\n",
    "\n",
    "6. **Threshold Selection:**\n",
    "   Determine a threshold above which features will be considered relevant. You can choose this threshold based on experimentation, domain knowledge, or by observing the distribution of feature importance scores.\n",
    "\n",
    "7. **Feature Selection:**\n",
    "   Select the features whose importance scores exceed the threshold. These are the features that the model considers most relevant for predicting soccer match outcomes.\n",
    "\n",
    "8. **Model Evaluation:**\n",
    "   Evaluate the model's performance using appropriate evaluation metrics such as accuracy, precision, recall, and F1-score. Perform cross-validation to ensure the model's generalization ability.\n",
    "\n",
    "9. **Iterate and Refine:**\n",
    "   If the model's performance is not satisfactory, you can consider iterating the process by adjusting the threshold for feature selection or by experimenting with different algorithms that provide feature importance scores.\n",
    "\n",
    "10. **Interpretability and Insights:**\n",
    "    One of the advantages of embedded methods is that they provide insights into which features are most influential in making predictions. This can help you understand which player statistics or team rankings have the strongest impact on match outcomes.\n",
    "\n",
    "Remember that while embedded methods are powerful for selecting relevant features, it's also a good practice to combine them with other techniques like filter methods and wrapper methods to thoroughly explore the best feature subset and achieve optimal predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a478dc-f98e-4c0a-9616-f2333f8b4f35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Q8. You are working on a project to predict the price of a house based on its features, such as size, location, and age. You have a limited number of features, and you want to ensure that you select the most important ones for the model. Explain how you would use the Wrapper method to select the best set of features for the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054fbdea-f415-4b36-a28b-99add3915850",
   "metadata": {},
   "source": [
    "The Wrapper Method is a feature selection technique that involves using a machine learning model as an auxiliary tool to evaluate different subsets of features and select the best-performing subset. In your project to predict house prices, you can use the Wrapper Method to systematically evaluate different combinations of features and select the best set. Here's how you can do it:\n",
    "\n",
    "1. **Feature Subset Generation:**\n",
    "   Generate all possible combinations of the limited number of features you have. This includes subsets with just one feature, two features, three features, and so on, up to the total number of features.\n",
    "\n",
    "2. **Model Selection:**\n",
    "   Choose a performance metric to evaluate the subsets of features. Common metrics include Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), or R-squared.\n",
    "\n",
    "3. **Cross-Validation Setup:**\n",
    "   Divide your dataset into training and validation sets. To avoid overfitting, perform k-fold cross-validation on the training set. For each fold, train the model on a subset of features and evaluate its performance on the validation fold.\n",
    "\n",
    "4. **Model Training and Evaluation:**\n",
    "   For each combination of features, train a machine learning model (such as a linear regression model, a random forest, or a gradient boosting model) on the training data using the selected features. Evaluate the model's performance on the validation data using the chosen performance metric.\n",
    "\n",
    "5. **Feature Subset Selection:**\n",
    "   Select the feature subset that results in the best performance metric. This subset of features is considered the best combination for predicting house prices based on the Wrapper Method.\n",
    "\n",
    "6. **Model Refinement:**\n",
    "   Once you've selected the best feature subset, you can further fine-tune the model parameters using the entire training dataset with the chosen subset of features.\n",
    "\n",
    "7. **Final Evaluation:**\n",
    "   Evaluate the final model's performance on a separate test dataset that the model hasn't seen before. This step provides an unbiased estimate of the model's generalization ability.\n",
    "\n",
    "8. **Interpretability and Insights:**\n",
    "   The Wrapper Method not only helps you select the best feature subset but also provides insights into which features are most influential in predicting house prices. This information can be valuable for understanding the factors that drive house prices in your dataset.\n",
    "\n",
    "9. **Iterate if Necessary:**\n",
    "   If the performance of the selected feature subset is not satisfactory, you might consider going back to step 1 and experimenting with different subsets or exploring different machine learning algorithms.\n",
    "\n",
    "Keep in mind that the Wrapper Method can be computationally intensive, especially if you have a large number of features. Therefore, it's important to strike a balance between exhaustively searching through all possible feature subsets and the computational resources available. If the number of features is too large, you might also consider using techniques like forward selection (adding features one by one) or backward elimination (removing features one by one) as variations of the Wrapper Method to make the process more manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8147fe52-7000-4f45-a422-7119596b2fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
